cff-version: 1.2.0
message: "If you use this benchmark in your research, please cite it as below."
type: software
title: "ChaosBench-Logic: A Benchmark for Evaluating Large Language Models on Complex Reasoning about Dynamical Systems"
version: 1.0.0
date-released: 2025-01-01
url: "https://github.com/11NOel11/ChaosBench-Logic"
repository-code: "https://github.com/11NOel11/ChaosBench-Logic"
license: MIT
authors:
  - family-names: "Thomas"
    given-names: "Noel"
    affiliation: "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)"
keywords:
  - "large language models"
  - "benchmark"
  - "dynamical systems"
  - "chaos theory"
  - "reasoning"
  - "evaluation"
  - "LLM"
  - "GPT-4"
  - "Claude"
  - "Gemini"
  - "LLaMA"
abstract: >
  ChaosBench-Logic is a comprehensive benchmark for evaluating Large Language
  Models (LLMs) on complex reasoning tasks involving chaotic and non-chaotic
  dynamical systems. The benchmark consists of 621 carefully curated questions
  spanning 30 dynamical systems (27 actively used, 3 reserved for extension) from
  physics, chemistry, biology, and mathematics. It tests models across 17 task types
  organized into 7 high-level reasoning categories: atomic facts, logical implications,
  multi-hop reasoning, cross-system comparison, domain-specific technical reasoning,
  counterfactual analysis, and multi-turn dialogue. The codebase supports multiple
  state-of-the-art LLMs (GPT-4, Claude-3.5, Gemini-2.5, LLaMA-3 70B, Mixtral,
  OpenHermes) in both zero-shot and chain-of-thought modes, providing comprehensive
  metrics including overall accuracy, dialogue accuracy, task-specific breakdowns,
  FOL violation detection, and bias analysis.
references:
  - type: article
    authors:
      - family-names: "Thomas"
        given-names: "Noel"
    title: "ChaosBench-Logic: Benchmarking Large Language Models on Complex Reasoning about Dynamical Systems"
    year: 2025
    journal: "arXiv preprint"
